import os
import sys

def setup_results_folder():
    """Create results folder structure for storing outputs and performance data."""
    results_dir = "results"
    
    # Create fresh folder structure if it doesn't exist
    # 'performance_data' will store JSON results from experiments
    # 'output_images' will store all processed images from the pipeline
    os.makedirs(os.path.join(results_dir, "performance_data"), exist_ok=True)
    os.makedirs(os.path.join(results_dir, "output_images"), exist_ok=True)
    
    return results_dir  # Return path for use in other functions

def zip_results(results_dir):
    """Create a zip archive of the results folder for easy download or sharing."""
    import zipfile
    
    zip_filename = "results.zip"
    
    try:
        # Walk through all files in the results directory and add them to the zip
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(results_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    # Store relative paths in zip to preserve folder structure
                    arcname = os.path.relpath(file_path, ".")
                    zipf.write(file_path, arcname)
        
        print(f"Results zip saved to: {zip_filename}")
        return zip_filename
        
    except Exception as e:
        # Catch any error during zipping and display
        print(f"Failed to create zip: {e}")
        return None

def run_all():
    """Run the complete parallel image processing pipeline."""
    print("=" * 60)
    print("PARALLEL IMAGE PROCESSING")
    print("=" * 60)
    
    # Step 0: Setup results folder structure
    results_dir = setup_results_folder()
    
    # Step 0.5: Verify dataset existence
    if not os.path.exists("food101_subset"):
        print("Dataset not found: food101_subset/")
        return  # Exit pipeline if dataset is missing
    
    # Step 0.6: Add src folder to path to import custom modules
    sys.path.append('src')
    from multiprocessing_impl import run_multiprocessing_experiment  # CPU-bound parallel implementation
    from concurrent_futures_impl import run_futures_experiment       # Alternative parallel implementation using futures
    from performance_analysis import plot_comparison                 # Analysis and plotting module
    
    # ---------------- STEP 1: Multiprocessing Implementation ---------------- #
    print("\n" + "=" * 60)
    print("STEP 1: Running Multiprocessing Implementation")
    print("=" * 60)
    mp_results = run_multiprocessing_experiment("food101_subset")
    # mp_results contains execution times for different numbers of processes
    
    # ---------------- STEP 2: Concurrent.Futures Implementation ---------------- #
    print("\n" + "=" * 60)
    print("STEP 2: Running Concurrent.Futures Implementation")
    print("=" * 60)
    futures_results = run_futures_experiment("food101_subset")
    # futures_results contains execution times for different numbers of workers
    
    # ---------------- STEP 3: Performance Analysis ---------------- #
    print("\n" + "=" * 60)
    print("STEP 3: Performance Analysis")
    print("=" * 60)
    # Generate plots and tables comparing execution time, speedup, and efficiency
    plot_comparison(mp_results, futures_results)
    
    # ---------------- STEP 4: Create downloadable zip ---------------- #
    print("\n" + "=" * 60)
    print("STEP 4: Download Files")
    print("=" * 60)
    # Create a zip of results folder for easy sharing
    zip_results(results_dir)
    
    # ---------------- Pipeline Complete ---------------- #
    print("\n" + "=" * 60)
    print("PROCESSING COMPLETE")
    print("=" * 60)
    print("\nGenerated files structure:")
    print("results/")  # Top-level folder
    print("├── performance_comparison.png        # Performance graphs")
    print("├── performance_data/                 # JSON results from experiments")
    print("└── output_images/                    # All processed images generated by pipeline")

if __name__ == "__main__":
    # Entry point: Run the entire pipeline
    run_all()
